"""
VietShare Crawler - Tá»± Ä‘á»™ng crawl + Ä‘Äƒng bÃ i lÃªn Firebase
"""

import json
import os
import re
import time
import hashlib
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from gemini_browser import GeminiBrowser, rewrite_with_url

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

CONFIG_FILE = os.path.join(os.path.dirname(__file__), 'config.json')
PROCESSED_FILE = os.path.join(os.path.dirname(__file__), 'processed.json')
FIREBASE_CRED = os.path.join(os.path.dirname(__file__), 'firebase-credentials.json')

# Khá»Ÿi táº¡o Firebase
db = None
def init_firebase():
    global db
    if db:
        return db
    try:
        if os.path.exists(FIREBASE_CRED):
            cred = credentials.Certificate(FIREBASE_CRED)
            firebase_admin.initialize_app(cred)
            db = firestore.client()
            print("ğŸ”¥ Firebase Ä‘Ã£ káº¿t ná»‘i!")
            return db
        else:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y firebase-credentials.json")
            return None
    except Exception as e:
        print(f"âŒ Lá»—i Firebase: {e}")
        return None

def publish_to_firebase(article):
    """ÄÄƒng bÃ i lÃªn Firestore"""
    global db
    if not db:
        db = init_firebase()
    if not db:
        return False
    
    try:
        # Kiá»ƒm tra slug Ä‘Ã£ tá»“n táº¡i
        existing = db.collection('articles').where('slug', '==', article['slug']).limit(1).get()
        if len(list(existing)) > 0:
            print(f"    âš ï¸ Slug Ä‘Ã£ tá»“n táº¡i")
            return False
        
        # ThÃªm vÃ o Firestore
        db.collection('articles').add(article)
        return True
    except Exception as e:
        print(f"    âŒ Lá»—i Ä‘Äƒng bÃ i: {e}")
        return False

def get_article_images(url):
    """Láº¥y thumbnail vÃ  táº¥t cáº£ áº£nh tá»« bÃ i viáº¿t gá»‘c"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        thumbnail = ''
        images = []
        
        # Thumbnail tá»« og:image (LUÃ”N CHÃNH XÃC NHáº¤T)
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            thumbnail = og_image['content']
        
        # Selector cá»¥ thá»ƒ cho tá»«ng trang
        # VnExpress: .fck_detail
        # Genk: .knc-content
        # 24h: .cate-24h-foot-arti-deta-info
        content_selectors = [
            '.fck_detail',           # VnExpress
            '.knc-content',          # Genk
            '.cate-24h-foot-arti-deta-info',  # 24h
            'article .content',
            '.article-content',
            '.content-detail',
            '.post-content',
            'article'
        ]
        
        content = None
        for sel in content_selectors:
            content = soup.select_one(sel)
            if content:
                break
        
        if content:
            for img in content.find_all('img'):
                src = img.get('data-src') or img.get('src') or img.get('data-original')
                if src:
                    # Bá» qua icon, logo, avatar, quáº£ng cÃ¡o
                    skip_keywords = ['icon', 'logo', 'avatar', 'emoji', 'pixel', 'adsense', 'tracking', 'gif', 'blank', 'lazy']
                    if any(x in src.lower() for x in skip_keywords):
                        continue
                    
                    # Fix URL
                    if not src.startswith('http'):
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            # Láº¥y domain tá»« URL gá»‘c
                            from urllib.parse import urlparse
                            parsed = urlparse(url)
                            src = f"{parsed.scheme}://{parsed.netloc}{src}"
                        else:
                            continue
                    
                    # Chá»‰ láº¥y áº£nh tá»« domain tin cáº­y
                    trusted = ['vnecdn.net', 'vnexpress', 'genk.vn', '24h.com.vn', 'quantrimang', 'kenh14', 'cafef']
                    if any(t in src.lower() for t in trusted):
                        images.append(src)
        
        # Náº¿u khÃ´ng cÃ³ thumbnail, dÃ¹ng áº£nh Ä‘áº§u tiÃªn
        if not thumbnail and images:
            thumbnail = images[0]
        
        return thumbnail, images
        
    except Exception as e:
        print(f"    âš ï¸ Lá»—i láº¥y áº£nh: {e}")
        return '', []

def get_thumbnail(url):
    """Láº¥y áº£nh thumbnail tá»« bÃ i viáº¿t gá»‘c (wrapper)"""
    thumb, _ = get_article_images(url)
    return thumb

def load_config():
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_processed():
    if os.path.exists(PROCESSED_FILE):
        with open(PROCESSED_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_processed(processed):
    with open(PROCESSED_FILE, 'w', encoding='utf-8') as f:
        json.dump(processed, f, ensure_ascii=False, indent=2)

def get_hash(url):
    return hashlib.md5(url.encode()).hexdigest()

def create_slug(title):
    """Táº¡o slug"""
    replacements = {
        'Ã ': 'a', 'Ã¡': 'a', 'áº¡': 'a', 'áº£': 'a', 'Ã£': 'a',
        'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº­': 'a', 'áº©': 'a', 'áº«': 'a',
        'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº·': 'a', 'áº³': 'a', 'áºµ': 'a',
        'Ã¨': 'e', 'Ã©': 'e', 'áº¹': 'e', 'áº»': 'e', 'áº½': 'e',
        'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»‡': 'e', 'á»ƒ': 'e', 'á»…': 'e',
        'Ã¬': 'i', 'Ã­': 'i', 'á»‹': 'i', 'á»‰': 'i', 'Ä©': 'i',
        'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'á»': 'o', 'Ãµ': 'o',
        'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»™': 'o', 'á»•': 'o', 'á»—': 'o',
        'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»£': 'o', 'á»Ÿ': 'o', 'á»¡': 'o',
        'Ã¹': 'u', 'Ãº': 'u', 'á»¥': 'u', 'á»§': 'u', 'Å©': 'u',
        'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»±': 'u', 'á»­': 'u', 'á»¯': 'u',
        'á»³': 'y', 'Ã½': 'y', 'á»µ': 'y', 'á»·': 'y', 'á»¹': 'y',
        'Ä‘': 'd', 'Ä': 'd'
    }
    slug = title.lower()
    for vn, en in replacements.items():
        slug = slug.replace(vn, en)
    slug = re.sub(r'[^a-z0-9\s-]', '', slug)
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug[:80]

def save_articles(articles):
    """LÆ°u bÃ i viáº¿t"""
    output_file = os.path.join(os.path.dirname(__file__), 'articles_to_import.json')
    existing = []
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            existing = json.load(f)
    existing.extend(articles)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(articles)} bÃ i vÃ o {output_file}")

def run_once(gemini, config, processed):
    """Cháº¡y má»™t vÃ²ng crawl"""
    all_articles = []
    
    for source in config['sources']:
        print(f"\nğŸ“¡ {source['name']}")
        
        try:
            feed = feedparser.parse(source['rss'])
            count = 0
            max_per_source = config['settings'].get('max_articles_per_source', 3)
            
            for entry in feed.entries:
                if count >= max_per_source:
                    break
                
                url_hash = get_hash(entry.link)
                if url_hash in processed:
                    continue
                
                print(f"  ğŸ“° {entry.title[:40]}...")
                print(f"     ğŸ”— {entry.link}")
                
                # Láº¥y áº£nh tá»« bÃ i gá»‘c TRÆ¯á»šC
                thumbnail, original_images = get_article_images(entry.link)
                print(f"     ğŸ“¸ TÃ¬m tháº¥y {len(original_images)} áº£nh tá»« bÃ i gá»‘c")
                
                # Gá»­i URL cho Gemini
                result = rewrite_with_url(entry.link, source['category'], gemini)
                
                if result and result.get('title') and result.get('content'):
                    now = datetime.now()
                    
                    # ChÃ¨n áº£nh gá»‘c vÃ o content
                    content = result['content']
                    if original_images:
                        parts = content.split('</p>')
                        if len(parts) > 1:
                            step = max(1, len(parts) // (len(original_images) + 1))
                            for i, img_url in enumerate(original_images[:5]):
                                insert_pos = min((i + 1) * step, len(parts) - 1)
                                parts[insert_pos] = parts[insert_pos] + f'<img src="{img_url}" alt="">'
                            content = '</p>'.join(parts)
                    
                    # Article cho lÆ°u local
                    article_local = {
                        'title': result['title'],
                        'slug': create_slug(result['title']),
                        'excerpt': result.get('excerpt', ''),
                        'content': content,
                        'category': source['category'],
                        'tags': result.get('tags', []),
                        'thumbnail': thumbnail,
                        'author': 'VietShare',
                        'views': 0,
                        'status': 'published',
                        'createdAt': now.isoformat(),
                        'updatedAt': now.isoformat(),
                        'publishedAt': now.isoformat()
                    }
                    
                    # Article cho Firebase
                    article_firebase = {
                        'title': result['title'],
                        'slug': create_slug(result['title']),
                        'excerpt': result.get('excerpt', ''),
                        'content': content,
                        'category': source['category'],
                        'tags': result.get('tags', []),
                        'thumbnail': thumbnail,
                        'author': 'VietShare',
                        'views': 0,
                        'status': 'published',
                        'createdAt': firestore.SERVER_TIMESTAMP,
                        'updatedAt': firestore.SERVER_TIMESTAMP,
                        'publishedAt': firestore.SERVER_TIMESTAMP
                    }
                    
                    all_articles.append(article_local)
                    processed.append(url_hash)
                    count += 1
                    print(f"     âœ… {result['title'][:40]}...")
                    
                    # LÆ¯U NGAY
                    save_articles([article_local])
                    save_processed(processed)
                    
                    # ÄÄ‚NG LÃŠN FIREBASE
                    if publish_to_firebase(article_firebase):
                        print(f"     ğŸ”¥ ÄÃ£ Ä‘Äƒng lÃªn website!")
                    else:
                        print(f"     ğŸ’¾ ÄÃ£ lÆ°u local")
                else:
                    print(f"     âš ï¸ Tháº¥t báº¡i")
                
                # Delay + new chat
                time.sleep(5)
                gemini.new_chat()
                
        except Exception as e:
            print(f"  âŒ Lá»—i: {e}")
    
    return len(all_articles), processed

def main():
    print("="*60)
    print("ğŸš€ VietShare Crawler - CONTINUOUS MODE")
    print("="*60)
    
    config = load_config()
    processed = load_processed()
    
    # Khá»Ÿi Ä‘á»™ng Gemini
    gemini = GeminiBrowser()
    if not gemini.start():
        print("âŒ KhÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c Gemini")
        return
    
    cycle = 0
    total_articles = 0
    
    try:
        while True:
            cycle += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”„ VÃ’NG {cycle} - Tá»•ng Ä‘Ã£ crawl: {total_articles} bÃ i")
            print(f"{'='*60}")
            
            # Reload config Ä‘á»ƒ cÃ³ thá»ƒ thay Ä‘á»•i nguá»“n khi Ä‘ang cháº¡y
            config = load_config()
            
            # Cháº¡y crawl
            new_count, processed = run_once(gemini, config, processed)
            total_articles += new_count
            
            if new_count > 0:
                print(f"\nâœ… VÃ²ng {cycle}: {new_count} bÃ i má»›i")
            else:
                print(f"\nğŸ“­ VÃ²ng {cycle}: KhÃ´ng cÃ³ bÃ i má»›i")
            
            # Kiá»ƒm tra continuous mode
            if not config['settings'].get('continuous_mode', False):
                print("\nğŸ›‘ Continuous mode OFF - Dá»«ng láº¡i")
                break
            
            # Äá»£i trÆ°á»›c khi cháº¡y vÃ²ng tiáº¿p
            wait_minutes = config['settings'].get('check_interval_minutes', 15)
            print(f"\nâ³ Äá»£i {wait_minutes} phÃºt trÆ°á»›c vÃ²ng tiáº¿p theo...")
            print(f"   (Ctrl+C Ä‘á»ƒ dá»«ng)")
            
            for i in range(wait_minutes * 60):
                time.sleep(1)
                if i % 60 == 0 and i > 0:
                    print(f"   CÃ²n {wait_minutes - i//60} phÃºt...")
                    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
        print(f"ğŸ“Š Tá»•ng cá»™ng Ä‘Ã£ crawl: {total_articles} bÃ i trong {cycle} vÃ²ng")
        
    finally:
        print("\nğŸ”’ ÄÃ³ng browser...")
        gemini.close()

if __name__ == '__main__':
    main()

